{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "This notebook focuses on continuing the training of the [sequence models](sequence_modeling.ipynb) (LSTM and RNN), evaluating their performance, and documenting the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n",
    "from tensorflow.keras.models import load_model\n",
    "from data_preparation import prepare_data_for_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_for_models()\n",
    "\n",
    "# Load pre-trained models\n",
    "lstm_model = load_model('../models/lstm_model.h5')\n",
    "rnn_model = load_model('../models/rnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Model Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Visualize Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../results/{model_name.lower()}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred, \n",
    "        'probabilities': y_pred_proba, \n",
    "        'true_labels': y_test  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 2s 15ms/step\n",
      "\n",
      "LSTM Model Evaluation:\n",
      "Accuracy: 0.9554\n",
      "Precision: 0.9565\n",
      "Recall: 0.9554\n",
      "F1-Score: 0.9552\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       236\n",
      "           1       0.98      0.98      0.98       851\n",
      "           2       0.97      0.96      0.96       153\n",
      "           3       0.97      0.96      0.96       123\n",
      "           4       0.93      0.82      0.87       294\n",
      "           5       0.97      0.98      0.98      1098\n",
      "\n",
      "    accuracy                           0.96      2755\n",
      "   macro avg       0.94      0.94      0.94      2755\n",
      "weighted avg       0.96      0.96      0.96      2755\n",
      "\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "\n",
      "RNN Model Evaluation:\n",
      "Accuracy: 0.6083\n",
      "Precision: 0.4838\n",
      "Recall: 0.6083\n",
      "F1-Score: 0.5282\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       236\n",
      "           1       0.71      0.68      0.70       851\n",
      "           2       0.84      0.69      0.76       153\n",
      "           3       0.00      0.00      0.00       123\n",
      "           4       0.00      0.00      0.00       294\n",
      "           5       0.55      0.90      0.68      1098\n",
      "\n",
      "    accuracy                           0.61      2755\n",
      "   macro avg       0.35      0.38      0.36      2755\n",
      "weighted avg       0.48      0.61      0.53      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models\n",
    "lstm_results = evaluate_model(lstm_model, X_test, y_test, 'LSTM')\n",
    "rnn_results = evaluate_model(rnn_model, X_test, y_test, 'RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_visualizations(lstm_results, rnn_results, y_test):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "    # 1. Performance Metrics Comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    lstm_scores = [\n",
    "        lstm_results['accuracy'], \n",
    "        lstm_results['precision'], \n",
    "        lstm_results['recall'], \n",
    "        lstm_results['f1_score']\n",
    "    ]\n",
    "    rnn_scores = [\n",
    "        rnn_results['accuracy'], \n",
    "        rnn_results['precision'], \n",
    "        rnn_results['recall'], \n",
    "        rnn_results['f1_score']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, lstm_scores, width, label='LSTM', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, rnn_scores, width, label='RNN', color='green', alpha=0.7)\n",
    "    plt.title('Model Performance Metrics Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, metrics, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Per-Class Precision Comparison\n",
    "    plt.subplot(2, 3, 2)\n",
    "    class_names = np.unique(y_test)\n",
    "    \n",
    "    # Compute per-class precision for both models\n",
    "    lstm_class_precision = classification_report(\n",
    "        y_test, \n",
    "        lstm_results['predictions'], \n",
    "        output_dict=True\n",
    "    )\n",
    "    rnn_class_precision = classification_report(\n",
    "        y_test, \n",
    "        rnn_results['predictions'], \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    lstm_precisions = [lstm_class_precision[str(cls)]['precision'] for cls in class_names]\n",
    "    rnn_precisions = [rnn_class_precision[str(cls)]['precision'] for cls in class_names]\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, lstm_precisions, width, label='LSTM', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, rnn_precisions, width, label='RNN', color='green', alpha=0.7)\n",
    "    plt.title('Per-Class Precision Comparison')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xticks(x, class_names, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # 3. Prediction Probability Distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    lstm_max_proba = np.max(lstm_results['probabilities'], axis=1)\n",
    "    rnn_max_proba = np.max(rnn_results['probabilities'], axis=1)\n",
    "    \n",
    "    plt.hist(lstm_max_proba, bins=50, alpha=0.5, label='LSTM', color='blue')\n",
    "    plt.hist(rnn_max_proba, bins=50, alpha=0.5, label='RNN', color='green')\n",
    "    plt.title('Prediction Probability Distribution')\n",
    "    plt.xlabel('Maximum Prediction Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    # 4. Misclassification Heatmap for LSTM\n",
    "    plt.subplot(2, 3, 4)\n",
    "    misclass_lstm = lstm_results['confusion_matrix'].astype('float') / lstm_results['confusion_matrix'].sum(axis=1)[:, np.newaxis]\n",
    "    np.fill_diagonal(misclass_lstm, 0)\n",
    "    \n",
    "    sns.heatmap(misclass_lstm, cmap='YlOrRd', annot=True, fmt='.2f')\n",
    "    plt.title('LSTM Misclassification Heatmap')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # 5. Misclassification Heatmap for RNN\n",
    "    plt.subplot(2, 3, 5)\n",
    "    misclass_rnn = rnn_results['confusion_matrix'].astype('float') / rnn_results['confusion_matrix'].sum(axis=1)[:, np.newaxis]\n",
    "    np.fill_diagonal(misclass_rnn, 0)\n",
    "    \n",
    "    sns.heatmap(misclass_rnn, cmap='YlOrRd', annot=True, fmt='.2f')\n",
    "    plt.title('RNN Misclassification Heatmap')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    plt.suptitle('Comprehensive Model Performance Analysis', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig('../results/model_performance_analysis.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alost\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alost\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alost\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "advanced_visualizations(lstm_results, rnn_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Metric\t\tLSTM\t\tRNN\n",
      "Accuracy\t0.9554\t\t0.6083\n",
      "Precision\t0.9565\t\t0.4838\n",
      "Recall\t\t0.9554\t\t0.6083\n",
      "F1-Score\t0.9552\t\t0.5282\n"
     ]
    }
   ],
   "source": [
    "def compare_models(lstm_results, rnn_results):\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"Metric\\t\\tLSTM\\t\\tRNN\")\n",
    "    print(f\"Accuracy\\t{lstm_results['accuracy']:.4f}\\t\\t{rnn_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision\\t{lstm_results['precision']:.4f}\\t\\t{rnn_results['precision']:.4f}\")\n",
    "    print(f\"Recall\\t\\t{lstm_results['recall']:.4f}\\t\\t{rnn_results['recall']:.4f}\")\n",
    "    print(f\"F1-Score\\t{lstm_results['f1_score']:.4f}\\t\\t{rnn_results['f1_score']:.4f}\")\n",
    "\n",
    "compare_models(lstm_results, rnn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'RNN'],\n",
    "    'Accuracy': [lstm_results['accuracy'], rnn_results['accuracy']],\n",
    "    'Precision': [lstm_results['precision'], rnn_results['precision']],\n",
    "    'Recall': [lstm_results['recall'], rnn_results['recall']],\n",
    "    'F1-Score': [lstm_results['f1_score'], rnn_results['f1_score']]\n",
    "})\n",
    "results_df.to_csv('../results/model_comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook continued the training of the LSTM and RNN models, evaluated their performance, and documented the results. The models were assessed using accuracy, precision, recall, and confusion matrix metrics. The training processes and results were visualized to provide a clear understanding of the models' performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
